{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "D0sO9ywxV7n2"
   },
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "% matplotlib inline\n",
    "\n",
    "import math\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.python.data import Dataset\n",
    "from datetime import datetime\n",
    "import dateparser\n",
    "import gc # garbage collector\n",
    "\n",
    "## Set some default values\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "path = '/media/alaa/Study/TalkingData_FraudDetection_challenge/data/talkingdata-adtracking-fraud-detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "q4KZ0DHqYJFV"
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "train_df = pd.read_csv(path+'train.csv', nrows=3000000, dtype=dtypes, #, nrows=1000000,\n",
    "                       usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "train_df = train_df.reindex(\n",
    "    np.random.permutation(train_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1520764035196,
     "user": {
      "displayName": "Alaa Khaled",
      "photoUrl": "//lh6.googleusercontent.com/-rb9wRS7twOk/AAAAAAAAAAI/AAAAAAAAA0o/rTq0xWbJzDY/s50-c-k-no/photo.jpg",
      "userId": "101885306512788460415"
     },
     "user_tz": -120
    },
    "id": "QBMpJxUomYa8",
    "outputId": "cd9618b8-fca0-4283-d7c1-9dcf9ec7437c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1228230</th>\n",
       "      <td>81514</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>115</td>\n",
       "      <td>2017-11-06 16:27:51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914685</th>\n",
       "      <td>93587</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>153</td>\n",
       "      <td>2017-11-06 16:47:15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085951</th>\n",
       "      <td>109674</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>134</td>\n",
       "      <td>2017-11-06 16:23:54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540415</th>\n",
       "      <td>11549</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>459</td>\n",
       "      <td>2017-11-06 16:10:55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362172</th>\n",
       "      <td>27815</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-06 16:06:57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ip  app  device  os  channel           click_time  is_attributed\n",
       "1228230   81514    3       1  19      115  2017-11-06 16:27:51              0\n",
       "1914685   93587   23       1   9      153  2017-11-06 16:47:15              0\n",
       "1085951  109674   18       1  20      134  2017-11-06 16:23:54              0\n",
       "540415    11549    6       1  17      459  2017-11-06 16:10:55              0\n",
       "362172    27815   12       1  19      245  2017-11-06 16:06:57              0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DXn7zTTBoi0g"
   },
   "outputs": [],
   "source": [
    "train_df.fillna(np.nan, inplace=True)\n",
    "train_df = train_df.loc[((train_df.ip.notnull()) & (train_df.app.notnull()) & \n",
    "                         (train_df.device.notnull()) & (train_df.os.notnull()) & (train_df.channel.notnull()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 231,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 587,
     "status": "error",
     "timestamp": 1520771745449,
     "user": {
      "displayName": "Alaa Khaled",
      "photoUrl": "//lh6.googleusercontent.com/-rb9wRS7twOk/AAAAAAAAAAI/AAAAAAAAA0o/rTq0xWbJzDY/s50-c-k-no/photo.jpg",
      "userId": "101885306512788460415"
     },
     "user_tz": -120
    },
    "id": "jZWTYbSRBfM-",
    "outputId": "17056386-5f3e-4d79-f9aa-df0cf8ded22d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group by...\n",
      "merge...\n"
     ]
    }
   ],
   "source": [
    "train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "\n",
    "print('group by...')\n",
    "gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'qty'})\n",
    "\n",
    "print('merge...')\n",
    "train_df = train_df.merge(gp, on=['ip','day','hour'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81514</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>115</td>\n",
       "      <td>2017-11-06 16:27:51</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93587</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>153</td>\n",
       "      <td>2017-11-06 16:47:15</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109674</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>134</td>\n",
       "      <td>2017-11-06 16:23:54</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11549</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>459</td>\n",
       "      <td>2017-11-06 16:10:55</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27815</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-06 16:06:57</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel           click_time  is_attributed  hour  \\\n",
       "0   81514    3       1  19      115  2017-11-06 16:27:51              0    16   \n",
       "1   93587   23       1   9      153  2017-11-06 16:47:15              0    16   \n",
       "2  109674   18       1  20      134  2017-11-06 16:23:54              0    16   \n",
       "3   11549    6       1  17      459  2017-11-06 16:10:55              0    16   \n",
       "4   27815   12       1  19      245  2017-11-06 16:06:57              0    16   \n",
       "\n",
       "   day   qty  \n",
       "0    6   372  \n",
       "1    6  1801  \n",
       "2    6   173  \n",
       "3    6    94  \n",
       "4    6    33  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1205,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      },
      {
       "item_id": 3
      },
      {
       "item_id": 4
      },
      {
       "item_id": 5
      },
      {
       "item_id": 6
      },
      {
       "item_id": 7
      },
      {
       "item_id": 8
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1176,
     "status": "ok",
     "timestamp": 1520765078087,
     "user": {
      "displayName": "Alaa Khaled",
      "photoUrl": "//lh6.googleusercontent.com/-rb9wRS7twOk/AAAAAAAAAAI/AAAAAAAAA0o/rTq0xWbJzDY/s50-c-k-no/photo.jpg",
      "userId": "101885306512788460415"
     },
     "user_tz": -120
    },
    "id": "aytg6IXiV7v9",
    "outputId": "a337142c-99fc-447a-c5ff-6f3a43ed8f73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>hour</th>\n",
       "      <th>qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2100000.0</td>\n",
       "      <td>2100000.0</td>\n",
       "      <td>2100000.0</td>\n",
       "      <td>2100000.0</td>\n",
       "      <td>2100000.0</td>\n",
       "      <td>2100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.4</td>\n",
       "      <td>27.8</td>\n",
       "      <td>23.4</td>\n",
       "      <td>246.9</td>\n",
       "      <td>16.2</td>\n",
       "      <td>574.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.5</td>\n",
       "      <td>282.5</td>\n",
       "      <td>56.8</td>\n",
       "      <td>125.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1767.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>675.0</td>\n",
       "      <td>3518.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15017.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            app    device        os   channel      hour       qty\n",
       "count 2100000.0 2100000.0 2100000.0 2100000.0 2100000.0 2100000.0\n",
       "mean       12.4      27.8      23.4     246.9      16.2     574.5\n",
       "std        15.5     282.5      56.8     125.4       0.4    1767.6\n",
       "min         0.0       0.0       0.0       0.0      14.0       1.0\n",
       "25%         3.0       1.0      13.0     134.0      16.0      40.0\n",
       "50%        12.0       1.0      18.0     237.0      16.0     107.0\n",
       "75%        15.0       1.0      19.0     328.0      16.0     323.0\n",
       "max       675.0    3518.0     715.0     498.0      17.0   15017.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation examples summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>hour</th>\n",
       "      <th>qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900000.0</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>900000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.4</td>\n",
       "      <td>28.0</td>\n",
       "      <td>23.4</td>\n",
       "      <td>246.7</td>\n",
       "      <td>16.2</td>\n",
       "      <td>579.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.6</td>\n",
       "      <td>283.3</td>\n",
       "      <td>56.8</td>\n",
       "      <td>125.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>675.0</td>\n",
       "      <td>3507.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15017.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           app   device       os  channel     hour      qty\n",
       "count 900000.0 900000.0 900000.0 900000.0 900000.0 900000.0\n",
       "mean      12.4     28.0     23.4    246.7     16.2    579.5\n",
       "std       15.6    283.3     56.8    125.3      0.4   1786.0\n",
       "min        0.0      0.0      0.0      0.0     14.0      1.0\n",
       "25%        3.0      1.0     13.0    134.0     16.0     40.0\n",
       "50%       12.0      1.0     18.0    237.0     16.0    107.0\n",
       "75%       15.0      1.0     19.0    328.0     16.0    323.0\n",
       "max      675.0   3507.0    702.0    498.0     17.0  15017.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training targets summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_attributed\n",
       "count      2100000.0\n",
       "mean             0.0\n",
       "std              0.0\n",
       "min              0.0\n",
       "25%              0.0\n",
       "50%              0.0\n",
       "75%              0.0\n",
       "max              1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation targets summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_attributed\n",
       "count       900000.0\n",
       "mean             0.0\n",
       "std              0.0\n",
       "min              0.0\n",
       "25%              0.0\n",
       "50%              0.0\n",
       "75%              0.0\n",
       "max              1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Select features:\n",
    "def preprocess_features(train_df):\n",
    "  \"\"\"\n",
    "  Prepares input features from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the features to be used for the model, including\n",
    "    synthetic features.\n",
    "  \"\"\"\n",
    "#   A = pd.to_numeric(train_df[\"ip\"], errors='coerce')\n",
    "  B = pd.to_numeric(train_df[\"app\"], errors='coerce')\n",
    "  C = pd.to_numeric(train_df[\"device\"], errors='coerce')\n",
    "  D = pd.to_numeric(train_df[\"os\"], errors='coerce')\n",
    "  E = pd.to_numeric(train_df[\"channel\"], errors='coerce')\n",
    "  F = pd.to_numeric(train_df[\"hour\"], errors='coerce')\n",
    "#   G = pd.to_numeric(train_df[\"day\"], errors='coerce')\n",
    "  H = pd.to_numeric(train_df[\"qty\"], errors='coerce')\n",
    "#   selected_features = pd.concat([A,B,C,D,E,F,G,H], axis=1)\n",
    "  selected_features = pd.concat([B,C,D,E,F,H], axis=1)\n",
    "  processed_features = selected_features.astype(int).copy()\n",
    "  # Create a synthetic feature\n",
    "\n",
    "  return processed_features\n",
    "\n",
    "def preprocess_targets(train_df):\n",
    "  \"\"\"Prepares target features (i.e., labels) from California housing data set.\n",
    "  Args:\n",
    "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the target feature.\n",
    "  \"\"\"\n",
    "  output_targets = pd.DataFrame()\n",
    "  # Scale the target to be in units of thousands of dollars.\n",
    "  output_targets[\"is_attributed\"] = train_df[\"is_attributed\"].astype(float)\n",
    "  return output_targets\n",
    "\n",
    " # Choose the first 70000 (out of 100000) examples for training.\n",
    "training_examples = preprocess_features(train_df.head(2100000))\n",
    "training_targets = preprocess_targets(train_df.head(2100000))\n",
    "\n",
    "# Choose the last 30000 (out of 17000) examples for validation.\n",
    "validation_examples = preprocess_features(train_df.tail(900000))\n",
    "validation_targets = preprocess_targets(train_df.tail(900000))\n",
    "\n",
    "# Double-check that we've done the right thing.\n",
    "print \"Training examples summary:\"\n",
    "display.display(training_examples.describe())\n",
    "print \"Validation examples summary:\"\n",
    "display.display(validation_examples.describe())\n",
    "\n",
    "print \"Training targets summary:\"\n",
    "display.display(training_targets.describe())\n",
    "print \"Validation targets summary:\"\n",
    "display.display(validation_targets.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1520765091755,
     "user": {
      "displayName": "Alaa Khaled",
      "photoUrl": "//lh6.googleusercontent.com/-rb9wRS7twOk/AAAAAAAAAAI/AAAAAAAAA0o/rTq0xWbJzDY/s50-c-k-no/photo.jpg",
      "userId": "101885306512788460415"
     },
     "user_tz": -120
    },
    "id": "snqUd7jepeau",
    "outputId": "fd5dad22-c328-47af-b009-f3ed91bd6506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100000, 6)\n"
     ]
    }
   ],
   "source": [
    "print training_examples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1520765093159,
     "user": {
      "displayName": "Alaa Khaled",
      "photoUrl": "//lh6.googleusercontent.com/-rb9wRS7twOk/AAAAAAAAAAI/AAAAAAAAA0o/rTq0xWbJzDY/s50-c-k-no/photo.jpg",
      "userId": "101885306512788460415"
     },
     "user_tz": -120
    },
    "id": "4IjKp4b-pjJ4",
    "outputId": "5022dd29-6554-4b5e-bf81-f91300a80d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900000, 6)\n"
     ]
    }
   ],
   "source": [
    "print validation_examples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1520765093778,
     "user": {
      "displayName": "Alaa Khaled",
      "photoUrl": "//lh6.googleusercontent.com/-rb9wRS7twOk/AAAAAAAAAAI/AAAAAAAAA0o/rTq0xWbJzDY/s50-c-k-no/photo.jpg",
      "userId": "101885306512788460415"
     },
     "user_tz": -120
    },
    "id": "BYw--IUVqMmS",
    "outputId": "363ea88f-3e7d-45dd-cd95-cde3cf8f6a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100000, 1)\n"
     ]
    }
   ],
   "source": [
    "print training_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1520765094388,
     "user": {
      "displayName": "Alaa Khaled",
      "photoUrl": "//lh6.googleusercontent.com/-rb9wRS7twOk/AAAAAAAAAAI/AAAAAAAAA0o/rTq0xWbJzDY/s50-c-k-no/photo.jpg",
      "userId": "101885306512788460415"
     },
     "user_tz": -120
    },
    "id": "s3mivYLwqIv4",
    "outputId": "8b1c1c5b-3c08-4714-ff6a-c001e2682cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900000, 1)\n"
     ]
    }
   ],
   "source": [
    "print validation_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "uVVWKph7V7zk"
   },
   "outputs": [],
   "source": [
    "## Feature Scaling, Feature crosses, etc..\n",
    "def construct_feature_columns(input_features_num=None, input_features_cat=None,\n",
    "                              feature_type=['numerical','cat_with_identy',\n",
    "                                            'cat_with_hash_bucket','cat_with_vocab_file',\n",
    "                                            'cat_with_vocab_list']):\n",
    "  \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "  Args:\n",
    "    input_features_num: The names of the numerical input features to use.\n",
    "    input_features_cat: The names of the categorical input features to use.\n",
    "    feature_type(list): The type of the feature categorical_with_identity/ categorical_with_hasBucket/\n",
    "                                          categorical_with_vocabulary_file/ categorical_with_vocabulary_list/\n",
    "                                          numerical \n",
    "  Returns:\n",
    "    A set of feature columns\n",
    "  \"\"\" \n",
    "  if 'numerical' in feature_type:\n",
    "    feature_cols = set([tf.feature_column.numeric_column(my_feature)\n",
    "                          for my_feature in input_features_num])\n",
    "  elif 'cat_with_identy' in feature_type:\n",
    "    feature_cols.update(set([tf.feature_column.categorical_column_with_identity(my_feature)\n",
    "                          for my_feature in input_features_cat]))\n",
    "  elif 'cat_with_hash_bucket' in feature_type:\n",
    "    feature_cols.update(set([tf.feature_column.categorical_column_with_hash_bucket(my_feature)\n",
    "                          for my_feature in input_features_cat]))\n",
    "  elif 'cat_with_vocab_file' in feature_type:\n",
    "    feature_cols.update(set([tf.feature_column.categorical_column_with_vocabulary_file(my_feature)\n",
    "                          for my_feature in input_features_cat]))\n",
    "  elif 'cat_with_vocab_list' in feature_type:\n",
    "    feature_cols.update(set([tf.feature_column.categorical_column_with_vocabulary_list(my_feature)\n",
    "                          for my_feature in input_features_cat]))\n",
    "  return feature_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling and Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "wIjRl2DEvaH8"
   },
   "outputs": [],
   "source": [
    "def standarize_date(series):\n",
    "  \"\"\"A function to standarize data to only\n",
    "     date of the day, excluding the time value\"\"\"\n",
    "  return series.apply(lambda x: str(dateparser.parse(x, settings={'DATE_ORDER': 'YMD'}))[:10])\n",
    "\n",
    "# train_df['click_time_date'] =  standarize_date(train_df['click_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "crHWhH3PV77-"
   },
   "outputs": [],
   "source": [
    "def linear_scale(series):\n",
    "  min_val = series.min()\n",
    "  max_val = series.max()\n",
    "  scale = (max_val - min_val) / 2.0\n",
    "  return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n",
    "\n",
    "def normalize_linear_scale(examples_dataframe):\n",
    "  \"\"\"Returns a version of the input `DataFrame` that has \n",
    "     all its features normalized linearly.\"\"\"\n",
    "  return examples_dataframe.apply(linear_scale, axis=0)\n",
    "\n",
    "# normalized_dataframe = normalize_linear_scale(preprocess_features(california_housing_dataframe))\n",
    "# normalized_training_examples = normalized_dataframe.head(70000)\n",
    "# normalized_validation_examples = normalized_dataframe.tail(30000)\n",
    "\n",
    "def log_normalize(series):\n",
    "  return series.apply(lambda x:math.log(abs(x+1.0)))\n",
    "\n",
    "def clip(series, clip_to_min, clip_to_max):\n",
    "  return series.apply(lambda x:(\n",
    "    min(max(x, clip_to_min), clip_to_max)))\n",
    "\n",
    "def z_score_normalize(series):\n",
    "  mean = series.mean()\n",
    "  std_dv = series.std()\n",
    "  return series.apply(lambda x:(x - mean) / std_dv)\n",
    "\n",
    "def binary_threshold(series, threshold):\n",
    "  return series.apply(lambda x:(1 if x > threshold else 0))\n",
    "\n",
    "def normalize(examples_dataframe, norm_type, clip_to_min=None, clip_to_max=None, threshold=None):\n",
    "  \"\"\"\n",
    "  norm_type = 'log', 'clipping', 'z_score_norm', or 'binary_threshold'\n",
    "    clip_to_min & clip_to_max only used in case of norm_type='clipping'\n",
    "    threshold only used in case of norm_type='binary_threshold'\n",
    "    \n",
    "  Note: this function could be used to do different kind of normalization on different features\n",
    "  by calling it multiple time for every group of features while specifying norm_type\n",
    "  Returns a version of the input `DataFrame` that has all its features normalized.\"\"\"\n",
    "  #\n",
    "  # YOUR CODE HERE: Normalize the inputs.\n",
    "  if norm_type == 'log':\n",
    "    return examples_dataframe.apply(log_normalize, axis=0)\n",
    "  elif norm_type == 'clipping':\n",
    "    return examples_dataframe.apply(clip, args=(clip_to_min, clip_to_max), axis=0)\n",
    "  elif norm_type == 'z_score_norm':\n",
    "    return examples_dataframe.apply(z_score_normalize, axis=0)\n",
    "  elif norm_type == 'binary_threshold':\n",
    "    return examples_dataframe.apply(binary_threshold, args=(threshold), axis=0)\n",
    "\n",
    "# normalized_dataframe = normalize(preprocess_features(training_examples), norm_type='log')\n",
    "# normalized_dataframe = normalize(preprocess_features(california_housing_dataframe), clip_to_min=0, clip_to_max=1, norm_type='clipping')\n",
    "# normalized_dataframe = normalize(preprocess_features(training_examples), norm_type='z_score_norm')\n",
    "# normalized_dataframe = normalize(preprocess_features(california_housing_dataframe), threshold=[0.7], norm_type='binary_threshold')\n",
    "# normalized_training_examples = normalized_dataframe.head(70000)\n",
    "# normalized_validation_examples = normalized_dataframe.tail(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "vp5NGSK7kJaj"
   },
   "outputs": [],
   "source": [
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a linear regression model of one feature.\n",
    "  \n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
    " \n",
    "    # Construct a dataset, and configure batching/repeating\n",
    "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    # Shuffle the data, if specified\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9cTyiyRaVuqM"
   },
   "outputs": [],
   "source": [
    "def train_linear_classifier_model(\n",
    "    my_optimizer,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "\n",
    "  \"\"\"Trains a linear regression model of one feature.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  as well as a plot of the training and validation loss over time.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    training_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for training.\n",
    "    training_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for training.\n",
    "    validation_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for validation.\n",
    "    validation_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for validation.\n",
    "      \n",
    "  Returns:\n",
    "    A `LinearClassifier` object trained on the training data.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 20\n",
    "  steps_per_period = steps / periods\n",
    "  \n",
    "  # Create a linear classifier object.\n",
    "#   my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "  my_optimizer = my_optimizer\n",
    "#   my_optimizer = tf.train.FtrlOptimizer(learning_rate=0.1,\n",
    "#                                         l1_regularization_strength=0.01,\n",
    "#                                         l2_regularization_strength=0.1) # Add regulaization to prevent overfitting\n",
    "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "# linear_classifier = tf.estimator.LinearClassifier(feature_columns=construct_feature_columns(feature_type=['numerical'],\n",
    "#                                                                                 input_features_num=training_examples),\n",
    "#                                                     optimizer=my_optimizer)\n",
    "  \n",
    "  NN_classifier = tf.estimator.DNNClassifier(feature_columns=construct_feature_columns(feature_type=['numerical'],\n",
    "                                                                     input_features_num=training_examples),\n",
    "                                             hidden_units=hidden_units,\n",
    "                                             optimizer=my_optimizer,\n",
    "                                             dropout=0.0001,\n",
    "                                             config=tf.estimator.RunConfig().replace(save_summary_steps=10))\n",
    "  # Create input functions\n",
    "  training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets[\"is_attributed\"], \n",
    "                                          batch_size=batch_size)\n",
    "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                  training_targets[\"is_attributed\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                    validation_targets[\"is_attributed\"], \n",
    "                                                    num_epochs=1, \n",
    "                                                    shuffle=False)\n",
    "  \n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print \"Training model...\"\n",
    "  print \"LogLoss (on training data):\"\n",
    "  training_log_losses = []\n",
    "  validation_log_losses = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    NN_classifier.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "    # Take a break and compute predictions.    \n",
    "    training_probabilities = NN_classifier.predict(input_fn=predict_training_input_fn)\n",
    "    training_probabilities = np.array([item['probabilities'] for item in training_probabilities])\n",
    "    \n",
    "    validation_probabilities = NN_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "    validation_probabilities = np.array([item['probabilities'] for item in validation_probabilities])\n",
    "    \n",
    "    training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
    "    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
    "    # Occasionally print the current loss.\n",
    "    print \"  period %02d : %0.2f\" % (period, training_log_loss)\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_log_losses.append(training_log_loss)\n",
    "    validation_log_losses.append(validation_log_loss)\n",
    "  print \"Model training finished.\"\n",
    "  \n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"LogLoss\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"LogLoss vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(training_log_losses, label=\"training\")\n",
    "  plt.plot(validation_log_losses, label=\"validation\")\n",
    "  plt.legend()\n",
    "\n",
    "  return NN_classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FInd optimum learning rate using Fastai library\n",
    "# !pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 622,
     "output_extras": [
      {
       "item_id": 11
      },
      {
       "item_id": 12
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1488372,
     "status": "ok",
     "timestamp": 1520768628810,
     "user": {
      "displayName": "Alaa Khaled",
      "photoUrl": "//lh6.googleusercontent.com/-rb9wRS7twOk/AAAAAAAAAAI/AAAAAAAAA0o/rTq0xWbJzDY/s50-c-k-no/photo.jpg",
      "userId": "101885306512788460415"
     },
     "user_tz": -120
    },
    "id": "MVHFYgxTyG7n",
    "outputId": "3d16c7d1-6b1f-424a-b531-2e9a9230633a"
   },
   "outputs": [],
   "source": [
    "# lrs = [1e-3, 1e-4, 1e-5]\n",
    "# learned_models = []\n",
    "# for lr in lrs:\n",
    "#     NN_classifier = train_linear_classifier_model(\n",
    "#                                                     my_optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=lr,\n",
    "#                                                                                                    l1_regularization_strength=0.001),\n",
    "#                                                     steps=1000,\n",
    "#                                                     batch_size=100,\n",
    "#                                                     training_examples=training_examples,\n",
    "#                                                     training_targets=training_targets,\n",
    "#                                                     validation_examples=validation_examples,\n",
    "#                                                     validation_targets=validation_targets)\n",
    "#     learned_models.append(NN_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "NN_classifier = train_linear_classifier_model(\n",
    "                                                my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.0001),\n",
    "                                                steps=2000,\n",
    "                                                batch_size=100,\n",
    "                                                hidden_units = [50,50],\n",
    "                                                training_examples=training_examples,\n",
    "                                                training_targets=training_targets,\n",
    "                                                validation_examples=validation_examples,\n",
    "                                                validation_targets=validation_targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NN_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-369ed004ad52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNN_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mNN_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NN_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "NN_classifier.lr_find()\n",
    "NN_classifier.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 381,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 191455,
     "status": "ok",
     "timestamp": 1520770173619,
     "user": {
      "displayName": "Alaa Khaled",
      "photoUrl": "//lh6.googleusercontent.com/-rb9wRS7twOk/AAAAAAAAAAI/AAAAAAAAA0o/rTq0xWbJzDY/s50-c-k-no/photo.jpg",
      "userId": "101885306512788460415"
     },
     "user_tz": -120
    },
    "id": "mocFxdpuV7_k",
    "outputId": "7d816551-212d-4ff9-b62e-279d64fe56bf"
   },
   "outputs": [],
   "source": [
    "predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                  validation_targets[\"is_attributed\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "\n",
    "evaluation_metrics = NN_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
    "\n",
    "print \"AUC on the validation set: %0.2f\" % evaluation_metrics['auc']\n",
    "print \"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy']\n",
    "\n",
    "validation_predictions = NN_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "# Get just the probabilities for the positive class\n",
    "validation_predictions = np.array([item['probabilities'][1] for item in validation_predictions])\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n",
    "    validation_targets, validation_predictions)\n",
    "plt.plot(false_positive_rate, true_positive_rate, label=\"our model\")\n",
    "plt.plot([0, 1], [0, 1], label=\"random classifier\")\n",
    "_ = plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "-Cs85QTP4ofW"
   },
   "outputs": [],
   "source": [
    "# path = 'data/talkingdata-adtracking-fraud-detection/'\n",
    "# test_df = pd.DataFrame([sub.split(\",\") for sub in data], columns=['ip','app','device','os','channel','click_time','attributed_time'])\n",
    "\n",
    "test_df = pd.read_csv(path + 'test.csv',dtype=dtypes, \n",
    "                      usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "test_df.fillna(np.nan, inplace=True)\n",
    "test_df = test_df.loc[((test_df.ip.notnull()) & (test_df.app.notnull()) &\n",
    "                       (test_df.device.notnull()) & (test_df.os.notnull()) & (test_df.channel.notnull()))]\n",
    "\n",
    "test_df['hour'] = pd.to_datetime(test_df.click_time).dt.hour.astype('uint8')\n",
    "test_df['day'] = pd.to_datetime(test_df.click_time).dt.day.astype('uint8')\n",
    "print('group by...')\n",
    "gp = test_df[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'qty'})\n",
    "print('merge...')\n",
    "test_df = test_df.merge(gp, on=['ip','day','hour'], how='left')# test_df = train_df.reindex("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_input_fn_test(features,  num_epochs=1, shuffle=False):\n",
    "     # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
    " \n",
    "    # Construct a dataset, and configure batching/repeating\n",
    "    ds = Dataset.from_tensor_slices((features)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    # Shuffle the data, if specified\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data\n",
    "    features = ds.make_one_shot_iterator().get_next()\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_examples = preprocess_features(test_df)\n",
    "print test_examples.head()\n",
    "print test_examples.shape\n",
    "predict_test_input_fn = lambda: my_input_fn_test(test_examples, \n",
    "                                            num_epochs=1, \n",
    "                                            shuffle=False)\n",
    "\n",
    "test_probabilities = NN_classifier.predict(input_fn = predict_test_input_fn)\n",
    "test_probabilities = np.array([item['probabilities'] for item in test_probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(test_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "print(\"Predicting...\")\n",
    "sub['is_attributed'] = test_probabilities\n",
    "filename = 'my_submission_1.csv'\n",
    "print(\"writing in...\", filename)\n",
    "sub.to_csv(filename, index=False)\n",
    "print(\"done...\")\n",
    "print(sub.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I need to normalize the training features or so for a better learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KZvXOFE_V72M"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def get_quantile_based_boundaries(feature_values, num_buckets):\n",
    "#   boundaries = np.arange(1.0, num_buckets) / num_buckets\n",
    "#   quantiles = feature_values.quantile(boundaries)\n",
    "#   return [quantiles[q] for q in quantiles.keys()]\n",
    "\n",
    "# def construct_feature_columns():\n",
    "#   \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "#   Returns:\n",
    "#     A set of feature columns\n",
    "#   \"\"\" \n",
    "#   households = tf.feature_column.numeric_column(\"households\")\n",
    "#   longitude = tf.feature_column.numeric_column(\"longitude\")\n",
    "#   latitude = tf.feature_column.numeric_column(\"latitude\")\n",
    "#   housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")\n",
    "#   median_income = tf.feature_column.numeric_column(\"median_income\")\n",
    "#   rooms_per_person = tf.feature_column.numeric_column(\"rooms_per_person\")\n",
    "  \n",
    "#   # Divide households into 7 buckets.\n",
    "#   bucketized_households = tf.feature_column.bucketized_column(\n",
    "#     households, boundaries=get_quantile_based_boundaries(\n",
    "#       training_examples[\"households\"], 7))\n",
    "\n",
    "#   # Divide longitude into 10 buckets.\n",
    "#   bucketized_longitude = tf.feature_column.bucketized_column(\n",
    "#     longitude, boundaries=get_quantile_based_boundaries(\n",
    "#       training_examples[\"longitude\"], 10))\n",
    "  \n",
    "#     # Divide latitude into 10 buckets\n",
    "#   bucketized_latitude = tf.feature_column.bucketized_column(\n",
    "#     latitude, boundaries=get_quantile_based_boundaries(\n",
    "#       training_examples[\"latitude\"], 10))\n",
    "  \n",
    "#   # Divide housing_median_age into  buckets\n",
    "#   bucketized_housing_median_age = tf.feature_column.bucketized_column(\n",
    "#     housing_median_age, boundaries=get_quantile_based_boundaries(\n",
    "#       training_examples[\"housing_median_age\"], 5))\n",
    "  \n",
    "#     # Divide median_income into  buckets\n",
    "#   bucketized_median_income = tf.feature_column.bucketized_column(\n",
    "#     median_income, boundaries=get_quantile_based_boundaries(\n",
    "#       training_examples[\"median_income\"], 10))\n",
    "\n",
    "#   # Divide rooms_per_person into  buckets\n",
    "#   bucketized_rooms_per_person =tf.feature_column.bucketized_column(\n",
    "#     rooms_per_person, boundaries=get_quantile_based_boundaries(\n",
    "#       training_examples[\"rooms_per_person\"], 10))\n",
    "  \n",
    "#   # YOUR CODE HERE: Make a feature column for the long_x_lat feature cross\n",
    "#   long_x_lat = tf.feature_column.crossed_column(\n",
    "#       set([bucketized_longitude, bucketized_latitude]), hash_bucket_size=1000)\n",
    "  \n",
    "#   feature_columns = set([\n",
    "#     bucketized_longitude,\n",
    "#     bucketized_latitude,\n",
    "#     bucketized_housing_median_age,\n",
    "#     bucketized_households,\n",
    "#     bucketized_median_income,\n",
    "#     bucketized_rooms_per_person,\n",
    "#     long_x_lat])\n",
    "  \n",
    "#   return feature_columns\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "my_model.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
